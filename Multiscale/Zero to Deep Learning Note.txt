
Data types : categorical , continues
             gender      , stock price

panda correcaltion df.corr() to explore redundant fiture on column and check if its useful

images, sound, text are unstructed data to process it to Learning is by extract its feature

image.ravel() -> flat image to 1 column
deep learning is automated feature learning

binary classification -> yes no
multi class classification -> A B C
regression -> used for continues values

learning process is building a hypothesis function ex. on linear reg y =b+Xw wehere weight -> slope , b -> offset

CostFunc (Error) in linear regresssion -> MSE
Training -> minimzing Cost
Minimum cost -> best model

train, test split + validation
Train score >> test score = overfitting ! caused by:
not presserving label ratio
not randomly sampling dataset
test set too small
train set too small

supervised = data, label, hypothesis, cost
overfitting -> generalization failure

cross validation LOLO and LPLO when there is subgroup in data (Generalize on users, leave some users out)
crossvalidation using scikit x keras

accuracy = overall
precision = test positive, how often prediction correct? TP/ Test yes
recall = actual value positive, TP/ actual yes
F1 = 2PR/(P+R)
scikit.learn classification report
sck.learn minmax scaler value 0-1
        standdart scaller mean center at 0 SD 1
pd.dummies -> convert categorical into binary column
        

logistic regression is classification (discrit) using sigmoid and cost function is average cross entropy (binary log loss) y = sigmoid(b+Xw)
linear regression is continues 

linear regression can be representated on neural network and give many input same as logistic regression 
linear x logistic regression diff in activation function (real value x sigmoid, perceptron)

regression with multiple output -> sigmoid activation on every output node (Non exclusive class)
regression with multiple output -> softmax last layer output value spread between 0-1 (mutual exclusive class)

tensorflow playground

activation function makes neural network non linear, non linearites are the secret of neural network (Sigmoid, Step, Tanh, ReLU, Softplus)

neural network is a FUNCTION

loss = binary_cossentropy tandem with activation=sigmoid
loss = categorical_cossentropy tandem with activation=softmax

derivatives -> slope, rate of change
gradient -> extend derivative to multivariate functions
gradient descent to minimize cost
back propagation -> update parameters
chain rule -> product of derivatives to calculate weight updates
backprop summary:
1. forward propagation
2. error in last layer
3. Backward propagation
4. Derivative of cost on wights
5. Derivative of cost on biases
6. Update weights

KERAS is API the back end can be tensorflow, scikit, pytorch etc.

model.add : 
    architecture
    input 
    output
    activation
    kernel_initializer (weight inits)
    kernel_regularizer (L1, L2 norm)
model.compile:
    optimizer
    learning rate
    lost (cost function)
    metrics (judge performace of model, ex['accuracy'])
model.summary:
    'print model summary'
model.fit:
    x_train, y_train
    epoch
    batch_size
    verbose (type 0, 1, 2)
    validation_split (from training set)
    validation_data (manual, X_test y_test)
    callbacks [checkpointer, earlystopper, tensorboard]
model.predict:
    X_test
model.evaluate:
    X_test. y_test
    verbose

np.argmax (sorting max output value)

increasing learning rate = converge faster
SGD is more acurate (than regular GD) but not optimal since its single update need to calculate gradient on all training data (one weight update per epoch) -> solution mini-batch GD (split epoch into small groups, power of 2 16,32,..) by averaging the group -> get the better estimation of the gradient, less noisy estimation of the gradient and do many update per epoch(speeding training process) 
big batch size -> slow convergency
small bacth size -> converging faster
intermediate bacth size -> medioker 
too smaller is noisy default is 32

SGD, SGD + Momentum, SGD+M+Nesterov  -> constant optimizer
AdaGrad,, RMSProp, Adam -> Adaptive optimizer
Recommended : Adam & RMSProp

Weight inizialitation is important [zeros, uniform, normal, he_normal, lecun_uniform]

when the dataset is not big enough compared to the complexity of the method used than weight inizialitation is matter

inner layer representation / visualize input output layer using model.Function([input], [output])

--Worth to try:--
Functional Keras API vs Sequential(default) API:
    more explicit but easy to customizes
Callback:
    EarlyStopping -> stop training if the val_loss doesn't improve
    ModelCheckpoint -> save the trained model to disk once training finished (save_best_only)
    Tensorboard -> outputing training information to a /tmp/ subdirectory a good process visualization
    
feature axtraction -> domain knowledge
convolutions -> automatically detect local pattern

more data or better model ? -> look the learning curve
batch normalaization -> enable higher learning rates, regularizes model (reduce overfitting), improves accuracy
dropout -> so its not too dependent
kernel reqularization (bias_r, activity_r)-> L1, L2

image transformation (data augmentation)-> more labeled data
color scaling
rotation
shift(up-down, left-right)
shear
zoom
flip(horizontal, vertical)
rescalle
add noise
occlusions
...
keras.prepocessing ImageDataGEnerator
gen.flow_from_directory

continuous traiing with data generated on-the-fly(data augmentation) untill reach good result or stop when its not improving. Epoch is not well defined so just define the batch_size

Hyperparameters:
Network architecture
    # Layers
    # Nodes
    Layer Types
    Activation function
Regularization
    Batch Norm:yes/no
    dropout:yes/no
    probability
    Weight regularization:yes/no
    wight inizialitation:Gaussian, Uniform, scaled,...
Data Augmentaion
    tRANSFORMATION
Optimizers
    type
    learning rate
    batch size

Hyperparameter optimization search:Grid search, random search, Bayesian optimizer

regularization technique:
    dimensionality reduction
    data augmentation
    dropout
    early stoppping
    collect more data
modern convolutional recommendation:
    use ReLU as activation
    use He et al. inizialitation
    try to add BatchNorm or dropout
    augmenting the training datas
